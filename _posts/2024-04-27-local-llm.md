---
layout: post
title:  "Local LLMs with llama.cpp"
tags: [technical, llm]
---

Most local LLM use is through some high-level application such as [SillyTavern](https://sillytavernai.com/). These offer advantages such as automatically templating input appropriately, setting context sizes, and potentially offering other features, such as image generation. They're often focused on chatbots, and giving certain personalities to the characters chatted with.

You might want to avoid that if you want less of a chat experience (i.e. more one-shot prompts), if you want some feature that's only available in the bleeding edge, or if you just want something lighter weight.

I opted for [llama.cpp](https://github.com/ggerganov/llama.cpp), but you could go for something more in-between such as [ollama](https://isaac-chung.github.io/blog/what-is-ollama) (and [Kobold](https://github.com/KoboldAI/KoboldAI-Client) users would drop down to [KoboldCPP](https://github.com/LostRuins/koboldcpp)).

# llama.cpp

Following [instructions](https://rentry.org/llama-mini-guide) (clone, make, download a model), you can use the tinyllama to generate some simple text:

```
llama.cpp -m ../tinyllama-1.1b-chat-v0.3.Q6_K.gguf -i -p "An article about the fall of Rome:\n\n" -n 200 -e
```

I note:

* token generation is fast
* quality is highly variable, but prompt tuning can make it very decent
* it doesn't read like ChatGPT (although probably still seems AI generated?)
* in interactive mode (`-i`), token generation doesn't stop -- after making an article, you'll get some ending text (e.g. copyright), then another article

Downloading another model (e.g. [Zephyr](https://huggingface.co/TheBloke/zephyr-7B-alpha-GGUF)) gives much slower generation, better quality, and also stops after a suitable amount of time.

# Chat templates

One possible reason for the lack of stop tokens on TinyLlama is that the input is incorrect: checking [the source](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF), you can see that desired input format is ChatML. That page tells you the exact format, but you can also derive them from [chujiezheng's `chat_templates`](https://github.com/chujiezheng/chat_templates/) (or the [HuggingFace transformers library](https://huggingface.co/docs/transformers/chat_templating)).

```
<|im_start|>system
{system_message}<|im_end|>
<|im_start|>user
{prompt}<|im_end|>
<|im_start|>assistant
```

Even using this template, it didn't often stop, possibly due to the low model quality.

Zephyr also has a particular input it does best with, but accepts (and does well with) pure user prompts as well.

```
<|system|>
{system}</s>
<|user|>
{user}</s>
<|assistant|>
```
